---
marp: true
header: "Travailler avec des Data Lakehouses en Python, sans Spark"
footer: "Romain Clement - PyConFR Lyon 2025"
paginate: true
theme: default
---

<!-- _header: "" -->
<!-- _footer: "" -->
<!-- _paginate: false -->

![bg right](./pydata-paris-2025-lakehouse/cover.png)

# Travailler avec des Data Lakehouses en Python, sans Spark

> **Romain Clement**

> PyConFR Lyon 2025
> 1er novembre 2025

<!--
Avez-vous dÃ©jÃ  dÃ©marrÃ© un cluster Spark juste pour mettre Ã  jour trois lignes dans une table Delta ?

Bonjour Ã  tous, je suis Romain. Pour les 30 prochaines minutes, nous allons plonger dans les workflows Data Lakehouse avec du Python uniquement, sans Spark.

Avant de dÃ©marrer, qui est familier avec le concept de Data Lakehouse ?
-->

---

## ğŸ’¡ Qu'est-ce qu'un Data Lakehouse ?

![Ã‰volution du Data Lakehouse](pydata-paris-2025-lakehouse/lakehouse_architecture.png)

<!--
Voici un diagramme rÃ©capitulatif par Databricks prÃ©sentant l'Ã©volution dans le temps de l'architecture d'analytique de donnÃ©es :

- depuis les data warehouses des annÃ©es 80,
- jusqu'Ã  l'Ã©mergence des data lakes dans les annÃ©es 2010 avec les cas d'usage de machine learning en tÃªte (besoin d'accÃ¨s au niveau des fichiers),
- jusqu'Ã  l'unification du data lakehouse dans l'Ã¨re 2020.
-->

---

![bg right:30% w:250px](pydata-paris-2025-lakehouse/lakehouse_formats.png)

## ğŸ’¡ Qu'est-ce qu'un Data Lakehouse ?

**Pont entre Data Lakes et Data Warehouses**

âœ… FlexibilitÃ© des Data Lakes
âœ… Gouvernance des Data Warehouses
âœ… SÃ©paration du stockage et calcul
âœ… Open Table Formats : Delta, Iceberg, Hudi, DuckLake

*PopularisÃ© par Databricks & Snowflake*

<!--
Alors, qu'est-ce qu'un Data Lakehouse exactement, et dans quel cadre cela est utile ?

Avez-vous dÃ©jÃ  eu du mal avec des lacs de donnÃ©es qui deviennent des marÃ©cages ?

Le concept de lakehouse comble ce fossÃ© entre Data Lakes et Data Warehouses en :
- Conservant la flexibilitÃ© des data lakes
- Apportant les garanties de cohÃ©rence des data warehouses
- DÃ©couplant le stockage et le calcul pour l'optimisation des coÃ»ts, avec notamment du stockage distribuÃ© dans le cloud comme S3 ou ADLS

Les formats de data lakehouse sont tous des formats de tables dits ouverts : on les dÃ©nomment "Open Table Formats". Vous avez probablement entendu parler de Delta Lake de Databricks, Iceberg de Netflix, Hudi d'Uber ou mÃªme le dernier en date, DuckLake de DuckDB.
-->

---

## âš™ï¸ FonctionnalitÃ©s Data Lakehouse

**Performance & CoÃ»t**
- Stockage et calcul scalables
- Dimensionnement indÃ©pendant

**FiabilitÃ© des donnÃ©es**
- Transactions ACID
- Application et Ã©volution de schÃ©ma

**FlexibilitÃ© opÃ©rationnelle**
- Time-travel
- Snapshots / branching

<!--
Alors, qu'obtenez-vous rÃ©ellement avec un lakehouse ? Laissez-moi mettre en avant les fonctionnalitÃ©s clÃ©s qui rÃ©solvent ces problÃ¨mes de marÃ©cages de donnÃ©es en combinant le meilleur des deux mondes : data lakes et data warehouses.

PremiÃ¨rement, vous obtenez les avantages de performance et de coÃ»t : des ressources de stockage et calcul que vous pouvez dimensionner indÃ©pendamment, plus besoin de payer pour des clusters de calcul inactifs.

DeuxiÃ¨mement, vous obtenez la fiabilitÃ© des donnÃ©es : grÃ¢ce aux transactions ACID les donnÃ©es ne sont plus corrompues lors des jobs Ã©chouÃ©s, et grÃ¢ce Ã  l'application de schÃ©ma prÃ©-dÃ©fini vos donnÃ©es restent cohÃ©rentes.

Et enfin, vous obtenez la flexibilitÃ© opÃ©rationnelle : le time-travel pour le dÃ©bogage et l'audit, les snapshots pour l'expÃ©rimentation. Des fonctionnalitÃ©s qui facilitent grandement votre vie en tant que data engineer ou data scientist.

Ce ne sont pas que des mots Ã  la mode : ce sont des solutions pratiques Ã  de vrais problÃ¨mes auxquels nous sommes confrontÃ©s rÃ©guliÃ¨rement.
-->

---

## ğŸ—‚ï¸ Exemple de format de donnÃ©es (Delta)

```
table_name
|-- _delta_log
|   |-- 00000000000000000000.json
|   |-- 00000000000000000001.json
|   |-- 00000000000000000002.json
|-- partition=1
    |-- part-00001-1a31a393-6db6-4d1a-bf4e-81ea061ff8cd-c000.snappy.parquet
|-- partition=2
    |-- part-00001-5af77102-9207-4c89-aaf6-37e1f815ec26-c000.snappy.parquet
    |-- part-00001-b11bab55-43d0-4d05-ae88-5b9481ae57db-c000.snappy.parquet
```

**Stockage = Fichiers Parquet + Journal transactionnel**

<!--
Laissez-moi vous montrer Ã  quoi ressemble rÃ©ellement un format Data Lakehouse sous le capot : prenons Delta Lake comme exemple.

Vos donnÃ©es sont stockÃ©es dans des fichiers Parquet : pensez "CSV mais en colonnes, compressÃ©, et avec des types de donnÃ©es."

La magie rÃ©side dans le rÃ©pertoire _delta_log : des fichiers JSON qui tracent chaque changement.
Ce journal transactionnel indique aux moteurs de requÃªte exactement quels fichiers Parquet lire pour une version donnÃ©e.

Cette simple combinaison vous donne les transactions ACID, le time-travel, et toutes ces fonctionnalitÃ©s dont nous venons d'Ã©voquer.

Pas d'infrastructure complexe : juste des fichiers sur un stockage avec lesquels n'importe quel moteur de requÃªte peut travailler.
-->

---

![bg right:40% w:300px](https://upload.wikimedia.org/wikipedia/commons/f/f3/Apache_Spark_logo.svg)

## ğŸ”¥ Pourquoi (ne pas utiliser) Spark ?

ConÃ§u pour les traitements de donnÃ©es massives (>= 1 To)

ComplexitÃ© d'infrastructure de cluster de calcul

Gestion de JVM

DÃ©veloppement local complexe

Qu'en est-il des donnÃ©es Ã  petite/moyenne Ã©chelle ?

<!--
Maintenant, voici le problÃ¨me : Spark est une technologie incroyable, c'est le systÃ¨me distribuÃ© de facto pour les traitements de donnÃ©es depuis 10 ans, mais cela a un coÃ»t.

Spark est conÃ§u pour des charges massives : nous parlons de centaines de gigaoctets, de tÃ©raoctets et au-delÃ . Mais Ã  quelle frÃ©quence traitez-vous rÃ©ellement autant de donnÃ©es ?

Le plus souvent, vous gÃ©rez la configuration de cluster de calcul, le tuning de la JVM, et un dÃ©veloppement local complexe juste pour mettre Ã  jour quelques centaines de lignes.

Et le pire : mÃªme lorsque votre stockage est important, votre requÃªte rÃ©elle ne touche peut-Ãªtre qu'un petit sous-ensemble de ces donnÃ©es.

Alors pourquoi ne pas commencer simplement ? Obtenez les avantages de stockage des formats lakehouse sans la complexitÃ© opÃ©rationnelle.

Vous pourrez toujours migrer sur Spark plus tard quand vous avez rÃ©ellement un besoin de mise Ã  l'Ã©chelle.
-->

---

## ğŸ DÃ©marrer en Python

| Format   | Librairie native | Polars | DuckDB |
| -------- | ---------------- | ------ | ------ |
| Delta    | `deltalake`      | âœ…      | âš ï¸      |
| Iceberg  | `pyiceberg`      | âœ…      | âš ï¸      |
| Hudi     | `hudi`           | -      | -      |
| DuckLake | -                | -      | âœ…      |

_Note : Ã©tat actuel en octobre 2025_

<!--
Alors, comment dÃ©marrer concrÃ¨tement ? L'Ã©cosystÃ¨me Python a explosÃ© avec le support lakehouse ces derniÃ¨res annÃ©es.

Pour chaque format, vous avez le choix : gestion de tables bas-niveau avec des librairies comme `deltalake` et `pyiceberg`, ou opÃ©rations plus haut niveau sur des dataframe avec `polars`.

Mais une autre approche Ã©mergente se rÃ©vÃ¨le tout aussi excitante : `duckdb` a un support partiel pour Delta et Iceberg, et bien sÃ»r un support complet pour DuckLake. Cependant, comme nous le verrons, vous pouvez le faire fonctionner avec n'importe quel format de table avec une astuce simple. Des performances simple-noeud qui rivalisent avec les systÃ¨mes distribuÃ©s, et il peut lire tous les formats lakehouse majeurs.

Nous nous concentrerons sur Delta Lake aujourd'hui car c'est le plus facile pour dÃ©marrer, mais tout ce que je vous montre peut Ãªtre adaptÃ© aux autres formats.

Le point clÃ© ici ? En 2025, il n'y a aucune raison pour que Spark soit votre choix par dÃ©faut pour les traitements de donnÃ©es Ã  base de lakehouse en Python.
-->

---

![bg right:40% w:300px](https://delta-io.github.io/delta-rs/delta-rust-no-whitespace.svg)

## Exemple Delta avec `deltalake`

**DÃ©monstration pratique**

âœ… CrÃ©er des tables & Ã©crire des donnÃ©es
âœ… OpÃ©rations de fusion
âœ… Historique et time-travel
âœ… Python pur, sans clusters

<!--
Passons maintenant Ã  la pratique avec un exemple simple utilisant des donnÃ©es mÃ©tÃ©o.

Nous allons parcourir les bases : crÃ©er des tables, Ã©crire des donnÃ©es, effectuer des fusions, et explorer les fonctionnalitÃ©s de time-travel de Delta.

Tout s'exÃ©cute localement avec juste Python - aucune configuration de cluster nÃ©cessaire.
-->

---

### CrÃ©er une table

```python
>>> from deltalake import DeltaTable, Field, Schema
>>>
>>> weather_table_uri = ".datasets/weather"
>>> table = DeltaTable.create(
        weather_table_uri,
        storage_options=None,
        schema=Schema(
            [
                Field("time", "timestamp"),
                Field("city", "string"),
                Field("temperature", "float"),
            ]
        ),
        name="Weather",
        description="Forecast weather data",
    )
```

<!--
CommenÃ§ons simplement : crÃ©er une table Delta consiste juste Ã  dÃ©finir un schÃ©ma et pointer vers un rÃ©pertoire.

Remarquez que nous utilisons des types de donnÃ©es appropriÃ©s : timestamp pour les donnÃ©es de temps, string pour les donnÃ©es catÃ©gorielles, et float pour les donnÃ©es quantitatives (mesures).

La librairie deltalake gÃ¨re tous les dÃ©tails du protocole Delta Lake pour nous.
 -->

---

### CrÃ©er une table

```bash
.datasets/weather
â””â”€â”€ _delta_log
    â””â”€â”€ 00000000000000000000.json
```

_RÃ©pertoire de table avec journal transactionnel initial_

<!--
Voici ce qui est crÃ©Ã© : juste un rÃ©pertoire avec un dossier _delta_log contenant le journal transactionnel.

Ce premier fichier JSON contient les donnÃ©es transactionnelles de la version initiale de la table, traÃ§ant l'historique de notre table.

Aucun fichier de donnÃ©es pour l'instant puisque nous n'avons encore rien Ã©crit.
-->

---

### Inspecter les mÃ©tadonnÃ©es de la table

```python
>>> str(table.metadata())
Metadata(
  id: '830c7cf1-f8f8-4c59-b3f7-369d93d914ca',
  name: Weather,
  description: 'Forecast weather data',
  partition_columns: [],
  created_time: 1758725496285,
  configuration: {}
)
```

<!--
Chaque table Delta possÃ¨de des mÃ©tadonnÃ©es associÃ©es. Remarquez l'ID unique autogÃ©nÃ©rÃ© et l'horodatage de crÃ©ation.

Les colonnes de partition vides signifient que nous ne partitionnons pas encore nos donnÃ©es, ce qui convient parfaitement pour les petits jeux de donnÃ©es.
-->

---

### Inspecter le schÃ©ma de la table

```python
>>> table.schema().to_arrow()
arro3.core.Schema
------------
time: Timestamp(Microsecond, Some("UTC"))
city: Utf8
temperature: Float32
```

<!--
Le schÃ©ma de la table est compatible avec le format Arrow, ce qui nous donne une large compatibilitÃ© avec l'Ã©cosystÃ¨me. Remarquez comment nos types Python sont mappÃ©s aux types Arrow.
 -->

---

### Ã‰crire dans une table

Ajoutons d'abord des donnÃ©es :

```python
>>> import pandas as pd
>>> from deltalake import write_deltalake
>>>
>>> weather_df_1 = pd.DataFrame(
  [
    {"time": "2025-09-30T12:00:00Z", "city": "Paris", "temperature": 10.0},
    {"time": "2025-09-30T13:00:00Z", "city": "Paris", "temperature": 11.0},
    {"time": "2025-09-30T14:00:00Z", "city": "Paris", "temperature": 12.0},
  ]
)
>>> write_deltalake(weather_table_uri, weather_df_1, mode="append", storage_options=None)
```

<!--
Ã‰crire des donnÃ©es dans une table est simple : crÃ©ez un DataFrame pandas (ou toute autre librairie DataFrame) et utilisez la fonction utilitaire write_deltalake.

Le mode='append' garantit que nous n'Ã©crasons pas les donnÃ©es existantes. Nous pouvons Ã©galement utiliser mode='overwrite' pour remplacer tout le contenu de la table.

En coulisse, cela crÃ©e des fichiers Parquet et met Ã  jour le journal transactionnel de maniÃ¨re atomique.
-->

---

### Ã‰crire dans une table

```bash
.datasets/weather
â”œâ”€â”€ _delta_log
â”‚   â”œâ”€â”€ 00000000000000000000.json
â”‚   â””â”€â”€ 00000000000000000001.json
â””â”€â”€ part-00001-4f6cdffe-981b-4157-b19b-7fba04b1f7a6-c000.snappy.parquet
```

_Nouvelle transaction et un fichier Parquet_

<!--
Nous voyons maintenant la puissance de Delta : une nouvelle entrÃ©e dans le journal transactionnel et un fichier Parquet contenant nos donnÃ©es.

Chaque opÃ©ration d'Ã©criture est tracÃ©e, nous donnant une piste d'audit complÃ¨te et permettant le time-travel.
-->

---

### Ã‰crire dans une table

Effectuons un upsert de donnÃ©es :

```python
>>> weather_df_2 = pd.DataFrame(
  [
    {"time": "2025-09-30T13:00:00Z", "city": "Paris", "temperature": 12.0},
    {"time": "2025-09-30T14:00:00Z", "city": "Paris", "temperature": 13.0},
    {"time": "2025-09-30T15:00:00Z", "city": "Paris", "temperature": 14.0},
  ]
)
>>> table = DeltaTable(weather_table_uri, storage_options=None)
>>> (
      table.merge(
        source=weather_df_2,
        source_alias="source",
        target_alias="target",
        predicate="target.time = source.time and target.city = source.city",
      )
      .when_matched_update(updates={"temperature": "source.temperature"})
      .when_not_matched_insert(
        updates={"time": "source.time", "city": "source.city", "temperature": "source.temperature"}
      )
      .execute()
    )
```

<!--
Voici une autre fonctionnalitÃ© puissante de Delta : l'opÃ©ration de fusion (merge), permettant par exemple d'effectuer des opÃ©rations de dÃ©duplication ou d'upserts.

Ici, nous mettons Ã  jour les prÃ©visions de tempÃ©rature existantes oÃ¹ time et city correspondent, et insÃ©rons les nouvelles. Cela serait complexe avec des fichiers Parquet bruts, mais Delta le gÃ¨re Ã©lÃ©gamment.
-->

---

### Ã‰crire dans une table

```bash
.datasets/weather
â”œâ”€â”€ _delta_log
â”‚   â”œâ”€â”€ 00000000000000000000.json
â”‚   â”œâ”€â”€ 00000000000000000001.json
â”‚   â””â”€â”€ 00000000000000000002.json
â”œâ”€â”€ part-00001-4f6cdffe-981b-4157-b19b-7fba04b1f7a6-c000.snappy.parquet
â”œâ”€â”€ part-00001-d7036469-24e9-4362-9871-9a3641365b29-c000.snappy.parquet
â””â”€â”€ part-00001-f06d4ec1-4545-4844-976c-c80d31bba1dd-c000.snappy.parquet
```

_Nouvelle transaction et deux fichiers Parquet_

<!--
AprÃ¨s la fusion, nous avons trois fichiers Parquet et trois entrÃ©es dans le journal transactionnel. Nous allons voir dans un instant pourquoi deux nouveaux fichiers au lieu d'un seul ont Ã©tÃ© crÃ©Ã©s.

Cela peut sembler beaucoup de fichiers, mais le planificateur de requÃªtes de Delta sait exactement quels fichiers contiennent quelles donnÃ©es pour des requÃªtes efficaces.
-->

---

```bash
.datasets/weather/part-00001-4f6cdffe-981b-4157-b19b-7fba04b1f7a6-c000.snappy.parquet
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           time           â”‚  city   â”‚  temperature   â”‚
â”‚ timestamp with time zone â”‚ varchar â”‚     float      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼
â”‚ 2025-09-30 12:00:00+00   â”‚ Paris   â”‚           10.0 â”‚
â”‚ 2025-09-30 13:00:00+00   â”‚ Paris   â”‚           11.0 â”‚
â”‚ 2025-09-30 14:00:00+00   â”‚ Paris   â”‚           12.0 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

.datasets/weather/part-00001-d7036469-24e9-4362-9871-9a3641365b29-c000.snappy.parquet
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           time           â”‚  city   â”‚  temperature   â”‚
â”‚ timestamp with time zone â”‚ varchar â”‚     float      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2025-09-30 13:00:00+00   â”‚ Paris   â”‚           12.0 â”‚
â”‚ 2025-09-30 14:00:00+00   â”‚ Paris   â”‚           13.0 â”‚
â”‚ 2025-09-30 12:00:00+00   â”‚ Paris   â”‚           10.0 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

.datasets/weather/part-00001-f06d4ec1-4545-4844-976c-c80d31bba1dd-c000.snappy.parquet
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           time           â”‚  city   â”‚  temperature   â”‚
â”‚ timestamp with time zone â”‚ varchar â”‚     float      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2025-09-30 15:00:00+00   â”‚ Paris   â”‚      14.0      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

<!--
Jetons un oeil aux fichiers Parquet pour comprendre ce qui s'est passÃ©.

Le premier fichier Parquet est l'insertion de donnÃ©es initiale avec le mode 'append'.

Ensuite, l'opÃ©ration de fusion a crÃ©Ã© deux nouveaux fichiers plutÃ´t que de modifier l'existant :
- Le premier contient les lignes mises Ã  jour
- Le second contient les lignes insÃ©rÃ©es.

C'est la clÃ© des garanties ACID de Delta. Les fichiers Parquet immuables signifient que les lecteurs concurrents ne sont jamais bloquÃ©s.
-->

---

### Lire une table

```python
>>> table = DeltaTable(weather_table_uri, storage_options=None)
>>> table.version()
2
>>> table.to_pandas()
                       time   city     temperature
0 2025-09-30 15:00:00+00:00  Paris            14.0
1 2025-09-30 13:00:00+00:00  Paris            12.0
2 2025-09-30 14:00:00+00:00  Paris            13.0
3 2025-09-30 12:00:00+00:00  Paris            10.0
```

<!--
Lire une table est plutÃ´t simple : Delta lit automatiquement tous les fichiers Parquet pertinents et vous donne la vue la plus rÃ©cente. Remarquez que nous sommes Ã  la version 2 maintenant, et les donnÃ©es reflÃ¨tent notre opÃ©ration de fusion avec des tempÃ©ratures mises Ã  jour.

L'objet DeltaTable permet de spÃ©cifier des conditions de filtrage et des noms de colonnes pour Ã©viter de lire tous les fichiers Parquet (c'est ce qu'on appelle les optimisations de file skipping, predicate pushdown et column pruning).
-->

---

### RÃ©cupÃ©rer l'historique de la table

Version 0 (crÃ©ation de la table) :

```python
>>> table.history()
[
  {
    'timestamp': 1758720246806,
    'operation': 'CREATE TABLE',
    'operationParameters': {
      'protocol': '{"minReaderVersion":1,"minWriterVersion":2}',
      'mode': 'ErrorIfExists',
      'location': 'file:///.../.datasets/weather',
      'metadata': '{"configuration":{},"createdTime":1758720246797...}'
    },
    'engineInfo': 'delta-rs:py-1.1.0',
    'clientVersion': 'delta-rs.py-1.1.0',
    'version': 0
  }
  ...
]
```

<!--
Pouvoir rÃ©cupÃ©rer l'historique de la table est prÃ©cieux pour le dÃ©bogage et l'audit des changements dans le temps.

Chaque opÃ©ration est tracÃ©e avec des mÃ©triques : combien de fichiers ont Ã©tÃ© impactÃ©s, temps d'exÃ©cution, lignes affectÃ©es. Ces mÃ©tadonnÃ©es opÃ©rationnelles nÃ©cessiteraient un effort d'ingÃ©nierie significatif pour Ãªtre implÃ©mentÃ©es Ã  partir de zÃ©ro.

Ceci est la version initiale de la table (version 0) avec les mÃ©tadonnÃ©es de crÃ©ation de table.
-->

---

### RÃ©cupÃ©rer l'historique de la table

Version 1 (ajout de donnÃ©es) :
```python
>>> table.history()
[
  ...
  {
    'timestamp': 1758720703062,
    'operation': 'WRITE',
    'operationParameters': {'mode': 'Append'},
    'engineInfo': 'delta-rs:py-1.1.0',
    'clientVersion': 'delta-rs.py-1.1.0',
    'operationMetrics': {
      'execution_time_ms': 142,
      'num_added_files': 1,
      'num_added_rows': 3,
      'num_partitions': 0,
      'num_removed_files': 0
    },
    'version': 1
  }
  ...
]
```

<!--
Ceci est la deuxiÃ¨me version de la table (version 1) avec l'opÃ©ration d'ajout de donnÃ©es.
 -->

---

### RÃ©cupÃ©rer l'historique de la table

Version 2 (fusion de donnÃ©es) :

```python
>>> table.history()
[
  ...
  {
    'timestamp': 1758726633699,
    'operation': 'MERGE',
    'operationParameters': {...},
    'readVersion': 1,
    'engineInfo': 'delta-rs:py-1.1.0',
    'operationMetrics': {
      'execution_time_ms': 45,
      'num_output_rows': 4,
      'num_source_rows': 3,
      'num_target_files_added': 2,
      'num_target_files_removed': 1,
      'num_target_files_scanned': 1,
      'num_target_files_skipped_during_scan': 0,
      'num_target_rows_copied': 1,
      'num_target_rows_deleted': 0,
      'num_target_rows_inserted': 1,
      'num_target_rows_updated': 2,
      'rewrite_time_ms': 10,
      'scan_time_ms': 0
    },
    'clientVersion': 'delta-rs.py-1.1.0',
    'version': 2
  }
]
```

<!--
Ceci est la troisiÃ¨me version de la table (version 2) avec l'opÃ©ration de fusion de donnÃ©es.
 -->

---

### Time-travel

```python
>>> table.load_as_version(0)
>>> table.to_pandas()
Empty DataFrame
Columns: [time, city, temperature]
Index: []

>>> table.load_as_version(1)
>>> table.to_pandas()
                       time   city     temperature
0 2025-09-30 12:00:00+00:00  Paris            10.0
1 2025-09-30 13:00:00+00:00  Paris            11.0
2 2025-09-30 14:00:00+00:00  Paris            12.0

>>> table.load_as_version(2)
>>> table.to_pandas()
                       time   city     temperature
0 2025-09-30 15:00:00+00:00  Paris            14.0
1 2025-09-30 13:00:00+00:00  Paris            12.0
2 2025-09-30 14:00:00+00:00  Paris            13.0
3 2025-09-30 12:00:00+00:00  Paris            10.0
```

<!--
Le time-travel est l'une des fonctionnalitÃ©s phares de Delta. Chargez n'importe quelle version prÃ©cÃ©dente pour le dÃ©bogage, la rÃ©cupÃ©ration de donnÃ©es, ou l'analyse reproductible.

Comme attendu, la version 0 est vide (juste aprÃ¨s la crÃ©ation de la table), la version 1 contient nos donnÃ©es initiales, la version 2 inclut la fusion. C'est pourquoi Delta conserve tous ces journaux transactionnels.
-->

---

![bg right:40% w:300px](https://duckdb.org/images/logo-dl/DuckDB_Logo-horizontal.svg)

## Exemple Delta avec `duckdb`

âœ… Scanner une table Delta
âœ… InteropÃ©rabilitÃ© avec `deltalake`

<!--
Maintenant que nous avons explorÃ© comment dÃ©marrer avec le package Python deltalake, passons Ã  DuckDB.

DuckDB apporte la sÃ©mantique SQL Ã  vos workflows lakehouse Python.

RequÃªtes rapides grÃ¢ce au multithreading efficace, configuration simple, et support natif Delta Lake : le complÃ©ment parfait Ã  deltalake pour l'exploration et l'analyse de donnÃ©es.
-->

---

### Scanner une table Delta

```sql
$ from delta_scan('.datasets/weather');
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           time           â”‚  city   â”‚  temperature   â”‚
â”‚ timestamp with time zone â”‚ varchar â”‚     float      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2025-09-30 15:00:00+00   â”‚ Paris   â”‚           14.0 â”‚
â”‚ 2025-09-30 13:00:00+00   â”‚ Paris   â”‚           12.0 â”‚
â”‚ 2025-09-30 14:00:00+00   â”‚ Paris   â”‚           13.0 â”‚
â”‚ 2025-09-30 12:00:00+00   â”‚ Paris   â”‚           10.0 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

<!--
C'est lÃ  que DuckDB brille : scan direct de table Delta sans aucune configuration.

Pointez simplement DuckDB vers votre rÃ©pertoire de table Delta et interrogez-la comme n'importe quelle base de donnÃ©es.

Remarquez que nous obtenons les mÃªmes donnÃ©es que nous avons crÃ©Ã©es prÃ©cÃ©demment, mais maintenant nous pouvons utiliser simplement du SQL. Pas d'ETL, pas de dÃ©placement de donnÃ©es, juste de la puissance analytique pure.

Ã€ noter que DuckDB a encore de nombreuses restrictions concernant les formats Lakehouse en octobre 2025 :
- Seulement des opÃ©rations de lecture, pas d'Ã©criture (Iceberg vient d'arriver)
- Les opÃ©rations de time-travel sont encore en dÃ©veloppement
-->

---

### InteropÃ©rabilitÃ© avec `deltalake`

```python
>>> import duckdb
>>>
>>> weather_ds = table.to_pyarrow_dataset()
>>> conn = duckdb.connect()
>>> conn.register("weather", weather_ds)
>>> conn.execute("select * from weather").df()
                       time   city     temperature
0 2025-09-30 15:00:00+00:00  Paris            14.0
1 2025-09-30 13:00:00+00:00  Paris            12.0
2 2025-09-30 14:00:00+00:00  Paris            13.0
3 2025-09-30 12:00:00+00:00  Paris            10.0
```

<!--
Mais la vraie magie rÃ©side dans l'interopÃ©rabilitÃ©. Prenez votre table Delta depuis deltalake, convertissez-la en dataset PyArrow, et enregistrez-la dans DuckDB.

Et vous pouvez Ã©galement faire l'inverse : effectuez des requÃªtes complexes sur des tables Delta dans DuckDB, exportez les rÃ©sultats sous forme de structure Arrow Record Batches, et streamez les rÃ©sultats pour les Ã©crire avec la fonction write_deltalake ! De cette faÃ§on, vous pouvez effectuer des pipelines de donnÃ©es out-of-core pour des jeux de donnÃ©es plus volumineux.

Maintenant vous avez le meilleur des deux mondes : les opÃ©rations ACID de Delta pour les Ã©critures, l'analytique ultra-rapide de DuckDB pour les lectures. C'est la promesse du lakehouse : un jeu de donnÃ©es, plusieurs moteurs, le tout sans la complexitÃ© de Spark.
-->

---

## ğŸ—¼ PrÃ©sentation de `laketower`

Application Python utilitaire local-first (CLI + Web)

Gestion simple des tables lakehouse

Licence OSS Apache-2

Pour commencer :
- `uvx laketower`
- https://github.com/datalpia/laketower

<!--
Dernier outil Python pour les formats lakehouse : laissez-moi vous prÃ©senter un projet sur lequel je travaille depuis quelques mois, "Laketower". Je dÃ©veloppe celui-ci pour rÃ©pondre Ã  mes propres besoins, car la plupart des applications de donnÃ©es haut niveau ne supportent pas les formats lakehouse.

C'est une application utilitaire Python open-source sous licence Apache-2, fournissant une interface en ligne de commande et une interface web pour explorer et gÃ©rer les tables lakehouse. Elle est local-first, compatible avec les tables stockÃ©es localement et Ã  distance. Elle ne supporte que Delta Lake pour l'instant, mais d'autres formats comme Iceberg et DuckLake viendront.

La maniÃ¨re la plus simple de commencer Ã  l'utiliser : "uvx laketower" (vous pouvez bien sÃ»r faire "pip install laketower")
 -->

---

## ğŸ—¼ PrÃ©sentation de `laketower`

![w:1000px](https://raw.githubusercontent.com/datalpia/laketower/refs/heads/main/docs/static/tables_overview.png)

---

## ğŸ—¼ PrÃ©sentation de `laketower`

![w:1000px](https://raw.githubusercontent.com/datalpia/laketower/refs/heads/main/docs/static/tables_view.png)

---

## ğŸ—¼ PrÃ©sentation de `laketower`

![w:1000px](https://raw.githubusercontent.com/datalpia/laketower/refs/heads/main/docs/static/tables_query.png)

---

## ğŸ—¼ PrÃ©sentation de `laketower`

![w:1000px](https://raw.githubusercontent.com/datalpia/laketower/refs/heads/main/docs/static/tables_import.png)

---

## ğŸ—¼ PrÃ©sentation de `laketower`

![w:1000px](https://raw.githubusercontent.com/datalpia/laketower/refs/heads/main/docs/static/queries_view.png)

---

## ğŸš€ Ã€ retenir

**Commencez simple, montez en puissance quand nÃ©cessaire**

âœ… BÃ©nÃ©fices du lakehouse sans la complexitÃ© Spark
âœ… Ã‰cosystÃ¨me Python riche et disponible aujourd'hui
âœ… PrÃªt pour la production pour les charges petites et moyennes

**Prochaines Ã©tapes :** Choisissez un format, choisissez une librairie, construisez quelque chose !

<!--
Voici donc ce qu'il faut retenir : vous n'avez pas besoin de sur-complexifier votre stack de donnÃ©es.

Commencez avec les librairies lakehouse Python pour vos charges de travail petites Ã  moyennes. Obtenez tous les bÃ©nÃ©fices : transactions ACID, time-travel, application de schÃ©ma, sans le fardeau opÃ©rationnel.

L'Ã©cosystÃ¨me est mature, les outils sont prÃªts, et vous pouvez commencer aujourd'hui.

Et quand vous avez rÃ©ellement besoin de traitement distribuÃ© massif ? C'est Ã  ce moment que vous passez Ã  Spark. Mais commencez simple.

Votre moi du futur vous remerciera de ne pas avoir lancÃ© de clusters de calcul pour mettre Ã  jour trois lignes.
-->

---

## Romain CLEMENT

Consultant indepedent, **Datalpia**

![bg right width:80%](datalpia-profile.png)

Co-organisateur Meetup Python Grenoble

ğŸŒ [datalpia.com](https://datalpia.com)
ğŸŒ [romain-clement.net](https://romain-clement.net)
ğŸ”— [linkedin.com/in/romainclement](https://www.linkedin.com/in/romainclement)

---

# ğŸ™‹ Questions ?

Merci ! Discutons !

<!--
Questions anticipÃ©es :

"Comment se connecter aux tables Delta sur Databricks ?"
  - `deltalake` fournit maintenant une intÃ©gration native de l'API Unity Catalog pour obtenir l'URI de stockage objet et le jeton d'accÃ¨s temporaire
  - mais nÃ©cessite des permissions spÃ©ciales Ã  activer dans l'espace de travail Databricks concernant le "credential vending"

"Quel est le seuil Ã  partir duquel je dois passer Ã  Spark ?"
  - Une bonne rÃ¨gle empirique serait 100Go sur un seul noeud
  - Certaines Ã©tudes sur internet montrent que DuckDB et Polars peuvent encore gÃ©rer cette plage sur une machine puissante
  - Mais si les donnÃ©es croissent continuellement et que vous devez vraiment en traiter une partie ou la totalitÃ©, Spark pourrait Ãªtre plus sage / plus pÃ©renne dans ce cas

"Est-ce prÃªt pour la production ?"
  - "Ã‡a dÃ©pend" de vos exigences
  - Librairies bas niveau : oui, elles sont matures, supportent la plupart des opÃ©rations mais ne sont pas encore aussi complÃ¨tes que leurs homologues Spark (par exemple `deltalake` ne supporte pas les "deletion vectors" en septembre 2025)
  - DuckDB : trÃ¨s bonnes performances, mais ne supporte que les opÃ©rations de lecture pour Delta (Iceberg supporte l'Ã©criture depuis octobre 2025), la fonctionnalitÃ© de time-travel est partiellement implÃ©mentÃ©e aujourd'hui

"La mise Ã  jour des tables crÃ©e beaucoup de fichiers Parquet, pas efficace pour les moteurs de requÃªte. Comment corriger ?"
  - Utilisez l'optimisation "data compaction"
  - Elle ne fera que marquer les fichiers de donnÃ©es comme "supprimÃ©s" dans le journal transactionnel mais ne les supprime pas physiquement
  - Pour supprimer ces fichiers "marquÃ©s pour suppression", vous avez besoin de l'opÃ©ration "data vaccuming"

"Qu'en est-il de la rÃ©tention des donnÃ©es ?"
  - Utilisez l'opÃ©ration "data vaccuming" pour supprimer les anciens fichiers au-delÃ  d'une pÃ©riode de rÃ©tention donnÃ©e et ceux marquÃ©s pour suppression (aprÃ¨s une opÃ©ration d'optimisation par exemple)

"Pourquoi crÃ©er beaucoup de fichiers Parquet au lieu de les mettre Ã  jour ?"
  - Parquet est un format de fichier immuable

"Peut-on avoir des clÃ©s primaires ?"
  - Contraintes disponibles : NOT NULL, CHECK

"Qu'en est-il du traitement out-of-core mais avec des tailles de donnÃ©es moyennes ?"
  - Vous pouvez lire et Ã©crire des donnÃ©es par lots avec deltalake et duckdb
  - Cela nÃ©cessite gÃ©nÃ©ralement d'utiliser le format PyArrow Dataset entre les deux et Ã§a fonctionne trÃ¨s bien

"Puis-je avoir accÃ¨s au flux de changements de donnÃ©es dans une table ?"
  - Oui, en utilisant le Change Data Feed (CDF)
  - Il doit Ãªtre activÃ© Ã  la crÃ©ation de la table en utilisant "delta.enableChangeDataFeed"

"Y a-t-il des benchmarks de performance comparant Spark vs Delta-rs ou DuckDB ?"
  - Je ne l'ai pas fait moi-mÃªme
  - Mais il y a une comparaison 2025 par Miles Cole disponible : https://milescole.dev/data-engineering/2025/06/30/Spark-v-DuckDb-v-Polars-v-Daft-Revisited.html
  - La conclusion est : Ã  partir de 100GB Ã  traiter, Spark est toujours roi, sinon utilisez Delta-rs, Polars et/ou DuckDB pour la rÃ©duction des coÃ»ts
-->

---

## ğŸ“š References

- [Evolution to the Data Lakehouse](https://www.databricks.com/blog/2021/05/19/evolution-to-the-data-lakehouse.html)
- [Explaining the History of Data Lakehouse](https://dev.to/pkutaj/explaining-the-history-of-data-lakehouse-504a)
- [Understanding Open Table Formats](https://delta.io/blog/open-table-formats/)
- [Deltalake Python](https://delta-io.github.io/delta-rs/)
- [PyIceberg](https://py.iceberg.apache.org/)
- [Hudi Python/Rust](https://hudi.apache.org/docs/python-rust-quick-start-guide)
- [DuckLake](https://ducklake.select/)
- [DuckDB](https://duckdb.org)
